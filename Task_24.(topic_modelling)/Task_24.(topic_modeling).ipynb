{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic\n",
    "\n",
    "[voted-kaggle-dataset](https://www.kaggle.com/canggih/voted-kaggle-dataset/version/2#voted-kaggle-dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'voted-kaggle-dataset.csv'\n",
    "df = pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts= 2,150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'These files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('len of texts= {:,}'.format(len(df)))\n",
    "index = 10\n",
    "df.loc[index, 'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Versions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>1241</td>\n",
       "      <td>Version 2,2016-11-05|Version 1,2016-11-03</td>\n",
       "      <td>crime\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>442,136 views</td>\n",
       "      <td>53,128 downloads</td>\n",
       "      <td>1,782 kernels</td>\n",
       "      <td>26 topics</td>\n",
       "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
       "      <td>The datasets contains transactions made by cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>1046</td>\n",
       "      <td>Version 10,2016-10-24|Version 9,2016-10-24|Ver...</td>\n",
       "      <td>association football\\neurope</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>299 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>396,214 views</td>\n",
       "      <td>46,367 downloads</td>\n",
       "      <td>1,459 kernels</td>\n",
       "      <td>75 topics</td>\n",
       "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>1024</td>\n",
       "      <td>Version 2,2017-09-28</td>\n",
       "      <td>film</td>\n",
       "      <td>CSV</td>\n",
       "      <td>44 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>446,255 views</td>\n",
       "      <td>62,002 downloads</td>\n",
       "      <td>1,394 kernels</td>\n",
       "      <td>46 topics</td>\n",
       "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
       "      <td>Background\\nWhat can we say about the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td>789</td>\n",
       "      <td>Version 2,2017-07-19|Version 1,2016-12-08</td>\n",
       "      <td>crime\\nterrorism\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>187,877 views</td>\n",
       "      <td>26,309 downloads</td>\n",
       "      <td>608 kernels</td>\n",
       "      <td>11 topics</td>\n",
       "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
       "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitcoin Historical Data</td>\n",
       "      <td>Bitcoin data at 1-min intervals from select ex...</td>\n",
       "      <td>Zielak</td>\n",
       "      <td>618</td>\n",
       "      <td>Version 11,2018-01-11|Version 10,2017-11-17|Ve...</td>\n",
       "      <td>history\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>119 MB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>146,734 views</td>\n",
       "      <td>16,868 downloads</td>\n",
       "      <td>68 kernels</td>\n",
       "      <td>13 topics</td>\n",
       "      <td>https://www.kaggle.com/mczielinski/bitcoin-his...</td>\n",
       "      <td>Context\\nBitcoin is the longest running and mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title  \\\n",
       "0  Credit Card Fraud Detection   \n",
       "1     European Soccer Database   \n",
       "2      TMDB 5000 Movie Dataset   \n",
       "3    Global Terrorism Database   \n",
       "4      Bitcoin Historical Data   \n",
       "\n",
       "                                            Subtitle  \\\n",
       "0  Anonymized credit card transactions labeled as...   \n",
       "1  25k+ matches, players & teams attributes for E...   \n",
       "2                Metadata on ~5,000 movies from TMDb   \n",
       "3  More than 170,000 terrorist attacks worldwide,...   \n",
       "4  Bitcoin data at 1-min intervals from select ex...   \n",
       "\n",
       "                          Owner  Votes  \\\n",
       "0  Machine Learning Group - ULB   1241   \n",
       "1                  Hugo Mathien   1046   \n",
       "2     The Movie Database (TMDb)   1024   \n",
       "3              START Consortium    789   \n",
       "4                        Zielak    618   \n",
       "\n",
       "                                            Versions  \\\n",
       "0          Version 2,2016-11-05|Version 1,2016-11-03   \n",
       "1  Version 10,2016-10-24|Version 9,2016-10-24|Ver...   \n",
       "2                               Version 2,2017-09-28   \n",
       "3          Version 2,2017-07-19|Version 1,2016-12-08   \n",
       "4  Version 11,2018-01-11|Version 10,2017-11-17|Ve...   \n",
       "\n",
       "                                        Tags Data Type    Size License  \\\n",
       "0                             crime\\nfinance       CSV  144 MB    ODbL   \n",
       "1               association football\\neurope    SQLite  299 MB    ODbL   \n",
       "2                                       film       CSV   44 MB   Other   \n",
       "3  crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
       "4                           history\\nfinance       CSV  119 MB     CC4   \n",
       "\n",
       "           Views          Download        Kernels     Topics  \\\n",
       "0  442,136 views  53,128 downloads  1,782 kernels  26 topics   \n",
       "1  396,214 views  46,367 downloads  1,459 kernels  75 topics   \n",
       "2  446,255 views  62,002 downloads  1,394 kernels  46 topics   \n",
       "3  187,877 views  26,309 downloads    608 kernels  11 topics   \n",
       "4  146,734 views  16,868 downloads     68 kernels  13 topics   \n",
       "\n",
       "                                                 URL  \\\n",
       "0     https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
       "1          https://www.kaggle.com/hugomathien/soccer   \n",
       "2    https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
       "3               https://www.kaggle.com/START-UMD/gtd   \n",
       "4  https://www.kaggle.com/mczielinski/bitcoin-his...   \n",
       "\n",
       "                                         Description  \n",
       "0  The datasets contains transactions made by cre...  \n",
       "1  The ultimate Soccer database for data analysis...  \n",
       "2  Background\\nWhat can we say about the success ...  \n",
       "3  Context\\nInformation on more than 170,000 Terr...  \n",
       "4  Context\\nBitcoin is the longest running and mo...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\",\n",
       " 'The ultimate Soccer database for data analysis and machine learning\\nWhat you get:\\n+25,000 matches\\n+10,000 players\\n11 European Countries with their lead championship\\nSeasons 2008 to 2016\\nPlayers and Teams\\' attributes* sourced from EA Sports\\' FIFA video game series, including the weekly updates\\nTeam line up with squad formation (X, Y coordinates)\\nBetting odds from up to 10 providers\\nDetailed match events (goal types, possession, corner, cross, fouls, cards etc...) for +10,000 matches\\n*16th Oct 2016: New table containing teams\\' attributes from FIFA !\\nOriginal Data Source:\\nYou can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from:\\nhttp://football-data.mx-api.enetscores.com/ : scores, lineup, team formation and events\\nhttp://www.football-data.co.uk/ : betting odds. Click here to understand the column naming system for betting odds:\\nhttp://sofifa.com/ : players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.\\nWhen you have a look at the database, you will notice foreign keys for players and matches are the same as the original data sources. I have called those foreign keys \"api_id\".\\nImproving the dataset:\\nYou will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games, national cups, Champion\\'s League and Europa League. Please ask me if you\\'re after a specific tournament.\\nPlease get in touch with me if you want to help improve this dataset.\\nCLICK HERE TO ACCESS THE PROJECT GITHUB\\nImportant note for people interested in using the crawlers: since I first wrote the crawling scripts (in python), it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players (\\'Player Spider\\') will not work until i\\'ve updated it.\\nExploring the data:\\nNow that\\'s the fun part, there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you:\\nThe Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win, Draw, Away Win). They get it right about 53% of the time. This is also what I\\'ve achieved so far using my own SVM. Though it may sound high for such a random sport game, you\\'ve got to know that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.\\nProbabilities vs Odds\\nWhen running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions.\\nExplore and visualize features\\nWith access to players and teams attributes, team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows, Guardiola himself may hire one of you some day!',\n",
       " \"Background\\nWhat can we say about the success of a movie before it is released? Are there certain companies (Pixar?) that have found a consistent formula? Given that major films costing over $100 million to produce can still flop, this question is more important than ever to the industry. Film aficionados might have different interests. Can we predict which films will be highly rated, whether or not they are a commercial success?\\nThis is a great place to start digging in to those questions, with data on the plot, cast, crew, budget, and revenues of several thousand films.\\nData Source Transfer Summary\\nWe (Kaggle) have removed the original version of this dataset per a DMCA takedown request from IMDB. In order to minimize the impact, we're replacing it with a similar set of films and data fields from The Movie Database (TMDb) in accordance with their terms of use. The bad news is that kernels built on the old dataset will most likely no longer work.\\nThe good news is that:\\nYou can port your existing kernels over with a bit of editing. This kernel offers functions and examples for doing so. You can also find a general introduction to the new format here.\\nThe new dataset contains full credits for both the cast and the crew, rather than just the first three actors.\\nActor and actresses are now listed in the order they appear in the credits. It's unclear what ordering the original dataset used; for the movies I spot checked it didn't line up with either the credits order or IMDB's stars order.\\nThe revenues appear to be more current. For example, IMDB's figures for Avatar seem to be from 2010 and understate the film's global revenues by over $2 billion.\\nSome of the movies that we weren't able to port over (a couple of hundred) were just bad entries. For example, this IMDB entry has basically no accurate information at all. It lists Star Wars Episode VII as a documentary.\\nData Source Transfer Details\\nSeveral of the new columns contain json. You can save a bit of time by porting the load data functions from this kernel.\\nEven in simple fields like runtime may not be consistent across versions. For example, previous dataset shows the duration for Avatar's extended cut while TMDB shows the time for the original version.\\nThere's now a separate file containing the full credits for both the cast and crew.\\nAll fields are filled out by users so don't expect them to agree on keywords, genres, ratings, or the like.\\nYour existing kernels will continue to render normally until they are re-run.\\nIf you are curious about how this dataset was prepared, the code to access TMDb's API is posted here.\\nNew columns:\\nhomepage\\nid\\noriginal_title\\noverview\\npopularity\\nproduction_companies\\nproduction_countries\\nrelease_date\\nspoken_languages\\nstatus\\ntagline\\nvote_average\\nLost columns:\\nactor_1_facebook_likes\\nactor_2_facebook_likes\\nactor_3_facebook_likes\\naspect_ratio\\ncast_total_facebook_likes\\ncolor\\ncontent_rating\\ndirector_facebook_likes\\nfacenumber_in_poster\\nmovie_facebook_likes\\nmovie_imdb_link\\nnum_critic_for_reviews\\nnum_user_for_reviews\\nOpen Questions About the Data\\nThere are some things we haven't had a chance to confirm about the new dataset. If you have any insights, please let us know in the forums!\\nAre the budgets and revenues all in US dollars? Do they consistently show the global revenues?\\nThis dataset hasn't yet gone through a data quality analysis. Can you find any obvious corrections? For example, in the IMDb version it was necessary to treat values of zero in the budget field as missing. Similar findings would be very helpful to your fellow Kagglers! (It's probably a good idea to keep treating zeros as missing, with the caveat that missing budgets much more likely to have been from small budget films in the first place).\\nInspiration\\nCan you categorize the films by type, such as animated or not? We don't have explicit labels for this, but it should be possible to build them from the crew's job titles.\\nHow sharp is the divide between major film studios and the independents? Do those two groups fall naturally out of a clustering analysis or is something more complicated going on?\\nAcknowledgements\\nThis dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies, actors and actresses, crew members, and TV shows. You can try it for yourself here.\",\n",
       " 'Context\\nInformation on more than 170,000 Terrorist Attacks\\nThe Global Terrorism Database (GTD) is an open-source database including information on terrorist attacks around the world from 1970 through 2016 (with annual updates planned for the future). The GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170,000 cases. The database is maintained by researchers at the National Consortium for the Study of Terrorism and Responses to Terrorism (START), headquartered at the University of Maryland. More Information\\nContent\\nGeography: Worldwide\\nTime period: 1970-2016, except 1993 (2017 in progress, publication expected June 2018)\\nUnit of analysis: Attack\\nVariables: >100 variables on location, tactics, perpetrators, targets, and outcomes\\nSources: Unclassified media articles (Note: Please interpret changes over time with caution. Global patterns are driven by diverse trends in particular regions, and data collection is influenced by fluctuations in access to media coverage over both time and place.)\\nDefinition of terrorism:\\n\"The threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation.\"\\nSee the GTD Codebook for important details on data collection methodology, definitions, and coding schema.\\nAcknowledgements\\nThe Global Terrorism Database is funded through START, by the US Department of State (Contract Number: SAQMMA12M1292) and the US Department of Homeland Security Science and Technology Directorate’s Office of University Programs (Award Number 2012-ST-061-CS0001, CSTAB 3.1). The coding decisions and classifications contained in the database are determined independently by START researchers and should not be interpreted as necessarily representing the official views or policies of the United States Government.\\nGTD Team\\nPublications\\nThe GTD has been leveraged extensively in scholarly publications, reports, and media articles. Putting Terrorism in Context: Lessons from the Global Terrorism Database, by GTD principal investigators LaFree, Dugan, and Miller investigates patterns of terrorism and provides perspective on the challenges of data collection and analysis. The GTD\\'s data collection manager, Michael Jensen, discusses important Benefits and Drawbacks of Methodological Advancements in Data Collection and Coding.\\nTerms of Use\\nUse of the data signifies your agreement to the following terms and conditions.\\nDefinitions: Within this section: \"GTD\" will refer to the Global Terrorism Database produced by the National Consortium for the Study of Terrorism and Responses to Terrorism. This includes the data and codebook, any auxiliary materials present, and the World Wide Web interface by which the data are presented. \"START\" will refer to the National Consortium for the Study of Terrorism and Responses to Terrorism, a United States Department of Homeland Security Center of Excellence based at the University of Maryland. \"USER\" denotes the individual or set of individuals who access the GTD, i.e. the data, codebook, any auxiliary materials, and the World Wide Web interface by which the data are presented. \"GTD representatives\" denotes any senior management staff of START, and any employee or representative of said organization whom senior management staff designate to represent START in dealings with the USER.\\nUsage Rights: Pursuant to this agreement, START grants the USER the non-exclusive, non-guaranteed right to search, browse, and view all contents of the GTD World Wide Web interface.\\nAuthorship: All contents of the GTD were assembled by representatives of START and do not purport to reflect the official position or data collections of the Department of Homeland Security or any other agency of the United States government.\\nAcknowledgement: All information sourced from the GTD should be acknowledged by the USER and cited as follows: \"National Consortium for the Study of Terrorism and Responses to Terrorism (START). (2017). Global Terrorism Database [Data file]. Retrieved from https://www.kaggle.com/START-UMD/gtd\"\\nUnauthorized Publication of the Data: No part of the GTD may be republished on any website or accessible for public download in any format without the express permission of a GTD staff member. In addition, no part of the GTD may be distributed for any commercial purpose, nor with the intent that the data be used in any commercial enterprise, without the express permission of a GTD staff member. START reserves the right to withhold this permission.\\nPenalties: Penalties for failure to comply with the terms of this agreement may result in loss of access to the GTD and the forfeiture of user privileges, in addition to any other appropriate legal remedies.\\nLimitation of Liability: Although every reasonable effort has been made to check sources and verify facts, START cannot guarantee that accounts reported in the open literature are complete and accurate. START shall not be held liable for any loss or damage caused by errors or omissions or resulting from any use, misuse, or alteration of GTD data by the USER. The USER should not infer any additional actions or results beyond what is presented in a GTD entry and specifically, the USER should not infer an individual associated with a particular incident was tried and convicted of terrorism or any other criminal offense. If new documentation about an event becomes available, an entry may be modified, as necessary and appropriate.\\nTermination of Rights: The GTD developers reserve the right to remove access to the GTD website from any particular IP address or set of IP addresses, or to remove the database entirely from public access, at their discretion. In such an event, all USER rights granted in this document are terminated.\\nTraining\\nSTART has released the first in a series of training modules designed to equip GTD users with the knowledge and tools to best leverage the database. This training module provides a general overview of the GTD, including the data collection process, uses of the GTD, and patterns of global terrorism. Participants will learn basic data handling and how to generate summary statistics from the GTD using PivotTables in Microsoft Excel.\\nQuestions?\\nFind answers to Frequently Asked Questions.\\nContact the GTD staff at gtd@start.umd.edu.',\n",
       " 'Context\\nBitcoin is the longest running and most well known cryptocurrency, first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange, with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks, and are thus \"chained\" together, serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market, bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining!\\nContent\\ncoincheckJPY_1-min_data_2014-10-31_to_2018-01-08.csv\\nbitflyerJPY_1-min_data_2017-07-04_to_2018-01-08.csv\\ncoinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv\\nbitstampUSD_1-min_data_2012-01-01_to_2018-01-08.csv\\nCSV files for select bitcoin exchanges for the time period of Jan 2012 to Jan 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields populated with NaNs. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk.\\nAcknowledgements and Inspiration\\nThe various exchange APIs, for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain, as well as its first execution via the bitcoin protocol. I\\'d also like to thank viewers like you! Can\\'t wait to see what code or insights you all have to share.\\nI am a lowly Ph.D. student who did this for fun in my meager spare time. If you find this data interesting and you can spare a coffee to fuel my science, send it my way and I\\'d be immensely grateful!\\n1kmWmcQa8qN9ZrdGfdkw8EHKBgugKBRcF',\n",
       " 'Context\\nFor the first time, Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16,000 responses and we learned a ton about who is working with data, what’s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.\\nTo share some of the initial insights from the survey, we’ve worked with the folks from The Pudding to put together this interactive report. They’ve shared all of the kernels used in the report here.\\nContent\\nThe data includes 5 files:\\nschema.csv: a CSV file with survey schema. This schema includes the questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv.\\nmultipleChoiceResponses.csv: Respondents\\' answers to multiple choice and ranking questions. These are non-randomized and thus a single row does correspond to all of a single user\\'s answers. -freeformResponses.csv: Respondents\\' freeform answers to Kaggle\\'s survey questions. These responses are randomized within a column, so that reading across a single row does not give a single user\\'s answers.\\nconversionRates.csv: Currency conversion rates (to USD) as accessed from the R package \"quantmod\" on September 14, 2017\\nRespondentTypeREADME.txt: This is a schema for decoding the responses in the \"Asked\" column of the schema.csv file.\\nKernel Awards in November\\nIn the month of November, we’re awarding $1000 a week for code and analyses shared on this dataset via Kaggle Kernels. Read more about this month’s Kaggle Kernels Awards and help us advance the state of machine learning and data science by exploring this one of a kind dataset.\\nMethodology\\nThis survey received 16,716 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents, we grouped them into a group named “Other” for anonymity.\\nWe excluded respondents who were flagged by our survey system as “Spam” or who did not answer the question regarding their employment status (this question was the first required question, so not answering it indicates that the respondent did not proceed past the 5th question in our survey).\\nMost of our respondents were found primarily through Kaggle channels, like our email list, discussion forums and social media channels.\\nThe survey was live from August 7th to August 25th. The median response time for those who participated in the survey was 16.4 minutes. We allowed respondents to complete the survey at any time during that window.\\nWe received salary data by first asking respondents for their day-to-day currency, and then asking them to write in either their total compensation.\\nWe’ve provided a csv with an exchange rate to USD for you to calculate the salary in US dollars on your own.\\nThe question was optional\\nNot every question was shown to every respondent. In an attempt to ask relevant questions to each respondent, we generally asked work related questions to employed data scientists and learning related questions to students. There is a column in the schema.csv file called \"Asked\" that describes who saw each question. You can learn more about the different segments we used in the schema.csv file and RespondentTypeREADME.txt in the data tab.\\nTo protect the respondents’ identity, the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further, the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker.',\n",
       " \"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\\nThe columns in this dataset are:\\nId\\nSepalLengthCm\\nSepalWidthCm\\nPetalLengthCm\\nPetalWidthCm\\nSpecies\",\n",
       " \"The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world.\\nHere's a list of the available indicators along with a list of the available countries.\\nFor example, this data includes the life expectancy at birth from many countries around the world:\\nThe dataset hosted here is a slightly transformed verion of the raw files available here to facilitate analytics.\",\n",
       " 'Actually, I prepare this dataset for students on my Deep Learning and NLP course.\\nBut I am also very happy to see kagglers play around with it.\\nHave fun!\\nDescription:\\nThere are two channels of data provided in this dataset:\\nNews data: I crawled historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by reddit users\\' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\\nStock data: Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\\nI provided three data files in .csv format:\\nRedditNews.csv: two columns The first column is the \"date\", and second column is the \"news headlines\". All news are ranked from top to bottom based on how hot they are. Hence, there are 25 lines for each date.\\nDJIA_table.csv: Downloaded directly from Yahoo Finance: check out the web page for more info.\\nCombined_News_DJIA.csv: To make things easier for my students, I provide this combined dataset with 27 columns. The first column is \"Date\", the second is \"Label\", and the following ones are news headlines ranging from \"Top1\" to \"Top25\".\\n=========================================\\nTo my students:\\nI made this a binary classification task. Hence, there are only two labels:\\n\"1\" when DJIA Adj Close value rose or stayed as the same;\\n\"0\" when DJIA Adj Close value decreased.\\nFor task evaluation, please use data from 2008-08-08 to 2014-12-31 as Training Set, and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split.\\nAnd, of course, use AUC as the evaluation metric.\\n=========================================\\n+++++++++++++++++++++++++++++++++++++++++\\nTo all kagglers:\\nPlease upvote this dataset if you like this idea for market prediction.\\nIf you think you coded an amazing trading algorithm,\\nfriendly advice\\ndo play safe with your own money :)\\n+++++++++++++++++++++++++++++++++++++++++\\nFeel free to contact me if there is any question~\\nAnd, remember me when you become a millionaire :P',\n",
       " \"This data set includes 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed. It has been of great use when teaching statistics to kids. With certain types you can also give a geeky introduction to machine learning.\\nThis are the raw attributes that are used for calculating how much damage an attack will do in the games. This dataset is about the pokemon games (NOT pokemon cards or Pokemon Go).\\nThe data as described by Myles O'Neill is:\\n#: ID for each pokemon\\nName: Name of each pokemon\\nType 1: Each pokemon has a type, this determines weakness/resistance to attacks\\nType 2: Some pokemon are dual type and have 2\\nTotal: sum of all stats that come after this, a general guide to how strong a pokemon is\\nHP: hit points, or health, defines how much damage a pokemon can withstand before fainting\\nAttack: the base modifier for normal attacks (eg. Scratch, Punch)\\nDefense: the base damage resistance against normal attacks\\nSP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)\\nSP Def: the base damage resistance against special attacks\\nSpeed: determines which pokemon attacks first each round\\nThe data for this table has been acquired from several different sites, including:\\npokemon.com\\npokemondb\\nbulbapeida\\nOne question has been answered with this database: The type of a pokemon cannot be inferred only by it's Attack and Deffence. It would be worthy to find which two variables can define the type of a pokemon, if any. Two variables can be plotted in a 2D space, and used as an example for machine learning. This could mean the creation of a visual example any geeky Machine Learning class would love.\"]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subtitle = df['Description'].astype(str).tolist()\n",
    "df_subtitle[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define custom vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(min_df=40, stop_words=&#x27;english&#x27;, token_pattern=&#x27;\\\\b\\\\w{3,}\\\\b&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(min_df=40, stop_words=&#x27;english&#x27;, token_pattern=&#x27;\\\\b\\\\w{3,}\\\\b&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(min_df=40, stop_words='english', token_pattern='\\\\b\\\\w{3,}\\\\b')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df = 40, stop_words='english', token_pattern = r\"\\b\\w{3,}\\b\")\n",
    "vectorizer.fit(df_subtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review feratures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features = 885\n",
      "\n",
      "['000' '100' '1000' '1st' '200' '2000' '2008' '2009' '2010' '2011' '2012'\n",
      " '2013' '2014' '2015' '2016' '2017' '500' 'able' 'access' 'accessible'\n",
      " 'according' 'account' 'accuracy' 'accurate' 'acknowledgements' 'acquired'\n",
      " 'act' 'action' 'active' 'activities' 'activity' 'actual' 'actually' 'add'\n",
      " 'added' 'addition' 'additional' 'address' 'administration' 'age']\n"
     ]
    }
   ],
   "source": [
    "print('len of features = {:,}\\n'.format(len(vectorizer.get_feature_names_out())))\n",
    "print(vectorizer.get_feature_names_out()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t2\n",
      "  (0, 63)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 108)\t1\n",
      "  (0, 127)\t1\n",
      "  (0, 130)\t3\n",
      "  (0, 132)\t2\n",
      "  (0, 139)\t1\n",
      "  (0, 169)\t3\n",
      "  (0, 179)\t1\n",
      "  (0, 191)\t1\n",
      "  (0, 195)\t1\n",
      "  (0, 198)\t3\n",
      "  (0, 201)\t4\n",
      "  (0, 202)\t1\n",
      "  (0, 206)\t1\n",
      "  (0, 222)\t1\n",
      "  (0, 224)\t1\n",
      "  (0, 267)\t1\n",
      "  (0, 272)\t1\n",
      "  :\t:\n",
      "  (2148, 817)\t3\n",
      "  (2148, 860)\t4\n",
      "  (2148, 881)\t2\n",
      "  (2149, 24)\t1\n",
      "  (2149, 113)\t1\n",
      "  (2149, 170)\t1\n",
      "  (2149, 172)\t1\n",
      "  (2149, 173)\t1\n",
      "  (2149, 198)\t2\n",
      "  (2149, 205)\t3\n",
      "  (2149, 229)\t1\n",
      "  (2149, 295)\t1\n",
      "  (2149, 332)\t1\n",
      "  (2149, 381)\t1\n",
      "  (2149, 457)\t1\n",
      "  (2149, 491)\t3\n",
      "  (2149, 497)\t2\n",
      "  (2149, 513)\t3\n",
      "  (2149, 712)\t2\n",
      "  (2149, 719)\t1\n",
      "  (2149, 748)\t1\n",
      "  (2149, 758)\t1\n",
      "  (2149, 802)\t1\n",
      "  (2149, 864)\t2\n",
      "  (2149, 881)\t2\n"
     ]
    }
   ],
   "source": [
    "newsgroup_data_vectorized = vectorizer.transform(df_subtitle)\n",
    "print(newsgroup_data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create gensim corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(11, 1),\n",
       "  (13, 1),\n",
       "  (21, 1),\n",
       "  (22, 2),\n",
       "  (63, 1),\n",
       "  (80, 1),\n",
       "  (82, 1),\n",
       "  (91, 1),\n",
       "  (108, 1),\n",
       "  (127, 1),\n",
       "  (130, 3),\n",
       "  (132, 2),\n",
       "  (139, 1),\n",
       "  (169, 3),\n",
       "  (179, 1),\n",
       "  (191, 1),\n",
       "  (195, 1),\n",
       "  (198, 3),\n",
       "  (201, 4),\n",
       "  (202, 1),\n",
       "  (206, 1),\n",
       "  (222, 1),\n",
       "  (224, 1),\n",
       "  (267, 1),\n",
       "  (272, 1),\n",
       "  (282, 4),\n",
       "  (283, 3),\n",
       "  (316, 1),\n",
       "  (327, 1),\n",
       "  (337, 1),\n",
       "  (346, 3),\n",
       "  (376, 1),\n",
       "  (378, 1),\n",
       "  (390, 1),\n",
       "  (421, 2),\n",
       "  (454, 1),\n",
       "  (483, 2),\n",
       "  (518, 1),\n",
       "  (519, 1),\n",
       "  (530, 1),\n",
       "  (544, 1),\n",
       "  (574, 1),\n",
       "  (606, 1),\n",
       "  (609, 1),\n",
       "  (652, 1),\n",
       "  (670, 1),\n",
       "  (674, 1),\n",
       "  (675, 1),\n",
       "  (700, 1),\n",
       "  (708, 1),\n",
       "  (772, 1),\n",
       "  (792, 2),\n",
       "  (801, 1),\n",
       "  (836, 1),\n",
       "  (841, 1),\n",
       "  (843, 1),\n",
       "  (845, 1),\n",
       "  (846, 1)],\n",
       " [(0, 3),\n",
       "  (6, 1),\n",
       "  (14, 2),\n",
       "  (17, 2),\n",
       "  (18, 2),\n",
       "  (43, 1),\n",
       "  (49, 1),\n",
       "  (55, 1),\n",
       "  (73, 5),\n",
       "  (84, 1),\n",
       "  (104, 1),\n",
       "  (108, 1),\n",
       "  (118, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (143, 3),\n",
       "  (146, 1),\n",
       "  (148, 1),\n",
       "  (154, 1),\n",
       "  (168, 1),\n",
       "  (175, 1),\n",
       "  (181, 1),\n",
       "  (193, 1),\n",
       "  (198, 10),\n",
       "  (199, 2),\n",
       "  (201, 4),\n",
       "  (205, 1),\n",
       "  (221, 1),\n",
       "  (232, 2),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (264, 1),\n",
       "  (267, 1),\n",
       "  (271, 3),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (281, 1),\n",
       "  (283, 1),\n",
       "  (302, 1),\n",
       "  (305, 5),\n",
       "  (306, 3),\n",
       "  (315, 1),\n",
       "  (319, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (340, 3),\n",
       "  (346, 3),\n",
       "  (351, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (364, 1),\n",
       "  (380, 2),\n",
       "  (384, 1),\n",
       "  (385, 1),\n",
       "  (386, 1),\n",
       "  (401, 1),\n",
       "  (405, 1),\n",
       "  (418, 2),\n",
       "  (421, 1),\n",
       "  (430, 1),\n",
       "  (431, 1),\n",
       "  (434, 1),\n",
       "  (449, 3),\n",
       "  (451, 1),\n",
       "  (454, 1),\n",
       "  (457, 2),\n",
       "  (467, 1),\n",
       "  (468, 4),\n",
       "  (485, 1),\n",
       "  (497, 1),\n",
       "  (505, 2),\n",
       "  (510, 1),\n",
       "  (530, 2),\n",
       "  (532, 1),\n",
       "  (534, 1),\n",
       "  (537, 1),\n",
       "  (547, 1),\n",
       "  (563, 1),\n",
       "  (564, 7),\n",
       "  (581, 1),\n",
       "  (582, 1),\n",
       "  (598, 1),\n",
       "  (599, 1),\n",
       "  (605, 1),\n",
       "  (608, 1),\n",
       "  (621, 1),\n",
       "  (626, 1),\n",
       "  (679, 1),\n",
       "  (684, 1),\n",
       "  (692, 1),\n",
       "  (695, 1),\n",
       "  (696, 2),\n",
       "  (709, 2),\n",
       "  (731, 2),\n",
       "  (732, 1),\n",
       "  (736, 1),\n",
       "  (768, 1),\n",
       "  (776, 4),\n",
       "  (777, 4),\n",
       "  (792, 2),\n",
       "  (812, 1),\n",
       "  (818, 1),\n",
       "  (822, 1),\n",
       "  (830, 1),\n",
       "  (831, 1),\n",
       "  (835, 2),\n",
       "  (841, 2),\n",
       "  (842, 1),\n",
       "  (844, 1),\n",
       "  (850, 1),\n",
       "  (856, 1),\n",
       "  (863, 1),\n",
       "  (869, 3),\n",
       "  (873, 1),\n",
       "  (880, 1)],\n",
       " [(1, 1),\n",
       "  (8, 1),\n",
       "  (17, 1),\n",
       "  (18, 2),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (36, 1),\n",
       "  (49, 2),\n",
       "  (55, 4),\n",
       "  (56, 2),\n",
       "  (82, 1),\n",
       "  (97, 1),\n",
       "  (99, 1),\n",
       "  (115, 1),\n",
       "  (136, 1),\n",
       "  (142, 3),\n",
       "  (148, 1),\n",
       "  (152, 1),\n",
       "  (166, 1),\n",
       "  (168, 1),\n",
       "  (169, 1),\n",
       "  (195, 1),\n",
       "  (198, 8),\n",
       "  (199, 2),\n",
       "  (201, 9),\n",
       "  (222, 1),\n",
       "  (232, 1),\n",
       "  (246, 2),\n",
       "  (249, 1),\n",
       "  (272, 4),\n",
       "  (273, 1),\n",
       "  (287, 1),\n",
       "  (288, 3),\n",
       "  (289, 1),\n",
       "  (299, 1),\n",
       "  (309, 1),\n",
       "  (312, 1),\n",
       "  (316, 1),\n",
       "  (318, 2),\n",
       "  (321, 1),\n",
       "  (322, 2),\n",
       "  (326, 1),\n",
       "  (328, 1),\n",
       "  (337, 1),\n",
       "  (350, 1),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (374, 1),\n",
       "  (376, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (388, 1),\n",
       "  (393, 1),\n",
       "  (395, 1),\n",
       "  (398, 2),\n",
       "  (399, 1),\n",
       "  (400, 2),\n",
       "  (401, 3),\n",
       "  (405, 1),\n",
       "  (410, 1),\n",
       "  (424, 1),\n",
       "  (431, 2),\n",
       "  (432, 2),\n",
       "  (434, 1),\n",
       "  (439, 1),\n",
       "  (440, 1),\n",
       "  (456, 2),\n",
       "  (482, 1),\n",
       "  (485, 3),\n",
       "  (505, 5),\n",
       "  (506, 2),\n",
       "  (525, 1),\n",
       "  (527, 4),\n",
       "  (530, 3),\n",
       "  (534, 1),\n",
       "  (557, 2),\n",
       "  (575, 1),\n",
       "  (577, 1),\n",
       "  (581, 1),\n",
       "  (587, 1),\n",
       "  (599, 1),\n",
       "  (601, 1),\n",
       "  (611, 1),\n",
       "  (622, 1),\n",
       "  (623, 1),\n",
       "  (624, 2),\n",
       "  (631, 1),\n",
       "  (656, 1),\n",
       "  (658, 1),\n",
       "  (668, 1),\n",
       "  (683, 1),\n",
       "  (706, 1),\n",
       "  (712, 1),\n",
       "  (719, 3),\n",
       "  (721, 2),\n",
       "  (722, 1),\n",
       "  (728, 1),\n",
       "  (731, 2),\n",
       "  (742, 1),\n",
       "  (750, 1),\n",
       "  (764, 1),\n",
       "  (782, 1),\n",
       "  (788, 1),\n",
       "  (792, 2),\n",
       "  (812, 1),\n",
       "  (817, 1),\n",
       "  (835, 1),\n",
       "  (836, 1),\n",
       "  (839, 1),\n",
       "  (840, 1),\n",
       "  (844, 1),\n",
       "  (849, 3),\n",
       "  (873, 1)],\n",
       " [(0, 2),\n",
       "  (1, 1),\n",
       "  (10, 1),\n",
       "  (14, 2),\n",
       "  (15, 2),\n",
       "  (18, 5),\n",
       "  (19, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (31, 1),\n",
       "  (35, 2),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (41, 1),\n",
       "  (49, 2),\n",
       "  (52, 1),\n",
       "  (67, 2),\n",
       "  (69, 1),\n",
       "  (71, 1),\n",
       "  (80, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (89, 1),\n",
       "  (109, 1),\n",
       "  (113, 1),\n",
       "  (119, 1),\n",
       "  (123, 1),\n",
       "  (140, 6),\n",
       "  (143, 1),\n",
       "  (148, 2),\n",
       "  (157, 1),\n",
       "  (160, 1),\n",
       "  (165, 1),\n",
       "  (167, 1),\n",
       "  (170, 1),\n",
       "  (171, 2),\n",
       "  (172, 2),\n",
       "  (198, 18),\n",
       "  (199, 10),\n",
       "  (214, 4),\n",
       "  (222, 1),\n",
       "  (237, 1),\n",
       "  (241, 1),\n",
       "  (247, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (270, 2),\n",
       "  (289, 1),\n",
       "  (295, 1),\n",
       "  (296, 1),\n",
       "  (299, 1),\n",
       "  (304, 1),\n",
       "  (309, 1),\n",
       "  (311, 1),\n",
       "  (318, 7),\n",
       "  (319, 1),\n",
       "  (325, 2),\n",
       "  (347, 1),\n",
       "  (359, 2),\n",
       "  (363, 3),\n",
       "  (364, 2),\n",
       "  (372, 2),\n",
       "  (373, 1),\n",
       "  (376, 4),\n",
       "  (386, 1),\n",
       "  (397, 1),\n",
       "  (399, 1),\n",
       "  (406, 1),\n",
       "  (419, 1),\n",
       "  (445, 1),\n",
       "  (461, 2),\n",
       "  (477, 3),\n",
       "  (488, 1),\n",
       "  (497, 4),\n",
       "  (505, 1),\n",
       "  (508, 3),\n",
       "  (510, 1),\n",
       "  (513, 2),\n",
       "  (519, 1),\n",
       "  (521, 1),\n",
       "  (522, 2),\n",
       "  (525, 2),\n",
       "  (529, 1),\n",
       "  (534, 1),\n",
       "  (542, 3),\n",
       "  (545, 3),\n",
       "  (552, 2),\n",
       "  (557, 1),\n",
       "  (569, 1),\n",
       "  (573, 1),\n",
       "  (585, 1),\n",
       "  (586, 3),\n",
       "  (596, 1),\n",
       "  (600, 1),\n",
       "  (611, 2),\n",
       "  (613, 2),\n",
       "  (614, 2),\n",
       "  (615, 2),\n",
       "  (619, 1),\n",
       "  (624, 2),\n",
       "  (644, 2),\n",
       "  (649, 1),\n",
       "  (656, 1),\n",
       "  (660, 1),\n",
       "  (662, 1),\n",
       "  (664, 1),\n",
       "  (666, 1),\n",
       "  (671, 2),\n",
       "  (675, 1),\n",
       "  (676, 1),\n",
       "  (679, 3),\n",
       "  (689, 1),\n",
       "  (697, 1),\n",
       "  (701, 1),\n",
       "  (709, 1),\n",
       "  (712, 2),\n",
       "  (729, 1),\n",
       "  (731, 1),\n",
       "  (732, 2),\n",
       "  (742, 15),\n",
       "  (745, 2),\n",
       "  (746, 3),\n",
       "  (748, 1),\n",
       "  (761, 4),\n",
       "  (764, 1),\n",
       "  (776, 1),\n",
       "  (779, 1),\n",
       "  (782, 3),\n",
       "  (792, 4),\n",
       "  (799, 1),\n",
       "  (807, 3),\n",
       "  (810, 1),\n",
       "  (825, 1),\n",
       "  (826, 3),\n",
       "  (828, 3),\n",
       "  (831, 1),\n",
       "  (834, 1),\n",
       "  (835, 4),\n",
       "  (836, 1),\n",
       "  (838, 9),\n",
       "  (839, 1),\n",
       "  (840, 1),\n",
       "  (841, 1),\n",
       "  (846, 2),\n",
       "  (851, 1),\n",
       "  (861, 3),\n",
       "  (862, 2),\n",
       "  (867, 3),\n",
       "  (876, 4),\n",
       "  (880, 1)],\n",
       " [(7, 1),\n",
       "  (10, 1),\n",
       "  (24, 1),\n",
       "  (30, 1),\n",
       "  (55, 2),\n",
       "  (89, 1),\n",
       "  (114, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (157, 1),\n",
       "  (166, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 1),\n",
       "  (194, 5),\n",
       "  (198, 7),\n",
       "  (230, 2),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (237, 1),\n",
       "  (262, 1),\n",
       "  (288, 1),\n",
       "  (290, 1),\n",
       "  (292, 1),\n",
       "  (302, 1),\n",
       "  (334, 1),\n",
       "  (338, 1),\n",
       "  (362, 1),\n",
       "  (380, 1),\n",
       "  (381, 1),\n",
       "  (385, 1),\n",
       "  (407, 1),\n",
       "  (431, 2),\n",
       "  (452, 1),\n",
       "  (459, 1),\n",
       "  (466, 2),\n",
       "  (483, 1),\n",
       "  (485, 1),\n",
       "  (499, 1),\n",
       "  (519, 1),\n",
       "  (525, 2),\n",
       "  (552, 1),\n",
       "  (557, 1),\n",
       "  (587, 1),\n",
       "  (589, 1),\n",
       "  (605, 1),\n",
       "  (613, 2),\n",
       "  (641, 2),\n",
       "  (642, 1),\n",
       "  (656, 1),\n",
       "  (661, 1),\n",
       "  (680, 1),\n",
       "  (684, 1),\n",
       "  (689, 1),\n",
       "  (694, 1),\n",
       "  (712, 1),\n",
       "  (715, 1),\n",
       "  (731, 1),\n",
       "  (772, 1),\n",
       "  (786, 1),\n",
       "  (792, 3),\n",
       "  (794, 1),\n",
       "  (831, 1),\n",
       "  (848, 1),\n",
       "  (855, 2),\n",
       "  (858, 1)]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = gensim.matutils.Sparse2Corpus(newsgroup_data_vectorized, documents_columns=False)\n",
    "[item for item in corpus][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create id2word dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{202: 'datasets',\n",
       " 169: 'contains',\n",
       " 191: 'credit',\n",
       " 708: 'september',\n",
       " 11: '2013',\n",
       " 267: 'european',\n",
       " 201: 'dataset',\n",
       " 519: 'occurred',\n",
       " 206: 'days',\n",
       " 337: 'highly',\n",
       " 574: 'positive',\n",
       " 130: 'class',\n",
       " 21: 'account',\n",
       " 378: 'input',\n",
       " 846: 'variables',\n",
       " 675: 'result',\n",
       " 390: 'issues',\n",
       " 609: 'provide',\n",
       " 530: 'original',\n",
       " 283: 'features',\n",
       " 82: 'background',\n",
       " 376: 'information',\n",
       " 198: 'data',\n",
       " 518: 'obtained',\n",
       " 792: 'time',\n",
       " 282: 'feature',\n",
       " 700: 'seconds',\n",
       " 836: 'used',\n",
       " 272: 'example',\n",
       " 179: 'cost',\n",
       " 421: 'learning',\n",
       " 674: 'response',\n",
       " 845: 'variable',\n",
       " 772: 'takes',\n",
       " 843: 'value',\n",
       " 108: 'case',\n",
       " 316: 'given',\n",
       " 22: 'accuracy',\n",
       " 841: 'using',\n",
       " 63: 'area',\n",
       " 132: 'classification',\n",
       " 139: 'collected',\n",
       " 670: 'research',\n",
       " 454: 'machine',\n",
       " 327: 'group',\n",
       " 346: 'http',\n",
       " 91: 'big',\n",
       " 483: 'mining',\n",
       " 224: 'detection',\n",
       " 222: 'details',\n",
       " 195: 'current',\n",
       " 544: 'past',\n",
       " 606: 'projects',\n",
       " 652: 'related',\n",
       " 801: 'topics',\n",
       " 80: 'available',\n",
       " 127: 'cite',\n",
       " 13: '2015',\n",
       " 199: 'database',\n",
       " 49: 'analysis',\n",
       " 0: '000',\n",
       " 468: 'matches',\n",
       " 564: 'players',\n",
       " 181: 'countries',\n",
       " 6: '2008',\n",
       " 14: '2016',\n",
       " 777: 'teams',\n",
       " 73: 'attributes',\n",
       " 850: 'video',\n",
       " 305: 'game',\n",
       " 709: 'series',\n",
       " 364: 'including',\n",
       " 831: 'updates',\n",
       " 776: 'team',\n",
       " 434: 'line',\n",
       " 175: 'coordinates',\n",
       " 221: 'detailed',\n",
       " 467: 'match',\n",
       " 271: 'events',\n",
       " 319: 'goal',\n",
       " 818: 'types',\n",
       " 193: 'cross',\n",
       " 505: 'new',\n",
       " 768: 'table',\n",
       " 168: 'containing',\n",
       " 731: 'source',\n",
       " 252: 'easily',\n",
       " 842: 'usually',\n",
       " 232: 'different',\n",
       " 863: 'websites',\n",
       " 140: 'collection',\n",
       " 598: 'processing',\n",
       " 457: 'make',\n",
       " 430: 'life',\n",
       " 251: 'easier',\n",
       " 148: 'commercial',\n",
       " 835: 'use',\n",
       " 55: 'api',\n",
       " 143: 'com',\n",
       " 692: 'scores',\n",
       " 880: 'www',\n",
       " 822: 'understand',\n",
       " 141: 'column',\n",
       " 306: 'games',\n",
       " 608: 'property',\n",
       " 449: 'look',\n",
       " 732: 'sources',\n",
       " 104: 'called',\n",
       " 485: 'missing',\n",
       " 844: 'values',\n",
       " 17: 'able',\n",
       " 43: 'algorithm',\n",
       " 361: 'include',\n",
       " 386: 'international',\n",
       " 497: 'national',\n",
       " 418: 'league',\n",
       " 736: 'specific',\n",
       " 856: 'want',\n",
       " 333: 'help',\n",
       " 360: 'improve',\n",
       " 18: 'access',\n",
       " 605: 'project',\n",
       " 315: 'github',\n",
       " 359: 'important',\n",
       " 510: 'note',\n",
       " 547: 'people',\n",
       " 384: 'interested',\n",
       " 696: 'scripts',\n",
       " 621: 'python',\n",
       " 118: 'changed',\n",
       " 146: 'comes',\n",
       " 695: 'script',\n",
       " 563: 'player',\n",
       " 873: 'work',\n",
       " 830: 'updated',\n",
       " 276: 'exploring',\n",
       " 302: 'fun',\n",
       " 451: 'lot',\n",
       " 380: 'insights',\n",
       " 534: 'overview',\n",
       " 537: 'page',\n",
       " 401: 'kernels',\n",
       " 812: 'try',\n",
       " 351: 'ideas',\n",
       " 581: 'predict',\n",
       " 131: 'classes',\n",
       " 340: 'home',\n",
       " 869: 'win',\n",
       " 679: 'right',\n",
       " 281: 'far',\n",
       " 334: 'high',\n",
       " 626: 'random',\n",
       " 405: 'know',\n",
       " 84: 'base',\n",
       " 582: 'predicting',\n",
       " 684: 'running',\n",
       " 431: 'like',\n",
       " 532: 'output',\n",
       " 264: 'estimate',\n",
       " 154: 'compare',\n",
       " 275: 'explore',\n",
       " 599: 'produce',\n",
       " 385: 'interesting',\n",
       " 205: 'day',\n",
       " 656: 'released',\n",
       " 115: 'certain',\n",
       " 152: 'companies',\n",
       " 456: 'major',\n",
       " 1: '100',\n",
       " 482: 'million',\n",
       " 623: 'question',\n",
       " 374: 'industry',\n",
       " 326: 'great',\n",
       " 557: 'place',\n",
       " 742: 'start',\n",
       " 624: 'questions',\n",
       " 764: 'summary',\n",
       " 399: 'kaggle',\n",
       " 658: 'removed',\n",
       " 849: 'version',\n",
       " 668: 'request',\n",
       " 527: 'order',\n",
       " 358: 'impact',\n",
       " 721: 'similar',\n",
       " 712: 'set',\n",
       " 288: 'fields',\n",
       " 782: 'terms',\n",
       " 506: 'news',\n",
       " 99: 'built',\n",
       " 432: 'likely',\n",
       " 322: 'good',\n",
       " 400: 'kernel',\n",
       " 273: 'examples',\n",
       " 309: 'general',\n",
       " 388: 'introduction',\n",
       " 299: 'format',\n",
       " 398: 'just',\n",
       " 439: 'listed',\n",
       " 56: 'appear',\n",
       " 8: '2010',\n",
       " 318: 'global',\n",
       " 23: 'accurate',\n",
       " 440: 'lists',\n",
       " 142: 'columns',\n",
       " 166: 'contain',\n",
       " 395: 'json',\n",
       " 722: 'simple',\n",
       " 587: 'previous',\n",
       " 719: 'shows',\n",
       " 249: 'duration',\n",
       " 706: 'separate',\n",
       " 289: 'file',\n",
       " 839: 'users',\n",
       " 246: 'don',\n",
       " 631: 'ratings',\n",
       " 683: 'run',\n",
       " 136: 'code',\n",
       " 577: 'posted',\n",
       " 750: 'status',\n",
       " 525: 'open',\n",
       " 788: 'things',\n",
       " 424: 'let',\n",
       " 622: 'quality',\n",
       " 287: 'field',\n",
       " 350: 'idea',\n",
       " 728: 'small',\n",
       " 381: 'inspiration',\n",
       " 817: 'type',\n",
       " 410: 'labels',\n",
       " 575: 'possible',\n",
       " 97: 'build',\n",
       " 393: 'job',\n",
       " 328: 'groups',\n",
       " 321: 'going',\n",
       " 24: 'acknowledgements',\n",
       " 312: 'generated',\n",
       " 601: 'product',\n",
       " 840: 'uses',\n",
       " 611: 'provides',\n",
       " 36: 'additional',\n",
       " 172: 'context',\n",
       " 876: 'world',\n",
       " 52: 'annual',\n",
       " 304: 'future',\n",
       " 363: 'includes',\n",
       " 552: 'period',\n",
       " 109: 'cases',\n",
       " 671: 'researchers',\n",
       " 761: 'study',\n",
       " 828: 'university',\n",
       " 170: 'content',\n",
       " 15: '2017',\n",
       " 614: 'publication',\n",
       " 397: 'june',\n",
       " 825: 'unit',\n",
       " 445: 'location',\n",
       " 477: 'media',\n",
       " 67: 'articles',\n",
       " 119: 'changes',\n",
       " 545: 'patterns',\n",
       " 810: 'trends',\n",
       " 542: 'particular',\n",
       " 649: 'regions',\n",
       " 31: 'actual',\n",
       " 508: 'non',\n",
       " 745: 'state',\n",
       " 569: 'political',\n",
       " 254: 'economic',\n",
       " 729: 'social',\n",
       " 214: 'department',\n",
       " 513: 'number',\n",
       " 689: 'science',\n",
       " 779: 'technology',\n",
       " 521: 'office',\n",
       " 10: '2012',\n",
       " 167: 'contained',\n",
       " 666: 'representing',\n",
       " 522: 'official',\n",
       " 826: 'united',\n",
       " 746: 'states',\n",
       " 325: 'government',\n",
       " 615: 'publications',\n",
       " 662: 'reports',\n",
       " 295: 'following',\n",
       " 160: 'conditions',\n",
       " 701: 'section',\n",
       " 644: 'refer',\n",
       " 600: 'produced',\n",
       " 585: 'present',\n",
       " 867: 'wide',\n",
       " 861: 'web',\n",
       " 586: 'presented',\n",
       " 113: 'center',\n",
       " 85: 'based',\n",
       " 838: 'user',\n",
       " 372: 'individual',\n",
       " 373: 'individuals',\n",
       " 461: 'management',\n",
       " 529: 'organization',\n",
       " 664: 'represent',\n",
       " 834: 'usage',\n",
       " 697: 'search',\n",
       " 851: 'view',\n",
       " 171: 'contents',\n",
       " 573: 'position',\n",
       " 41: 'agency',\n",
       " 296: 'follows',\n",
       " 347: 'https',\n",
       " 862: 'website',\n",
       " 19: 'accessible',\n",
       " 613: 'public',\n",
       " 247: 'download',\n",
       " 35: 'addition',\n",
       " 237: 'distributed',\n",
       " 619: 'purpose',\n",
       " 123: 'check',\n",
       " 660: 'reported',\n",
       " 157: 'complete',\n",
       " 676: 'results',\n",
       " 71: 'associated',\n",
       " 241: 'documentation',\n",
       " 270: 'event',\n",
       " 488: 'modified',\n",
       " 37: 'address',\n",
       " 807: 'training',\n",
       " 406: 'knowledge',\n",
       " 799: 'tools',\n",
       " 89: 'best',\n",
       " 596: 'process',\n",
       " 419: 'learn',\n",
       " 86: 'basic',\n",
       " 311: 'generate',\n",
       " 748: 'statistics',\n",
       " 69: 'asked',\n",
       " 165: 'contact',\n",
       " 255: 'edu',\n",
       " 407: 'known',\n",
       " 7: '2009',\n",
       " 234: 'digital',\n",
       " 642: 'recorded',\n",
       " 499: 'need',\n",
       " 641: 'record',\n",
       " 114: 'central',\n",
       " 466: 'market',\n",
       " 292: 'financial',\n",
       " 362: 'included',\n",
       " 338: 'historical',\n",
       " 194: 'csv',\n",
       " 290: 'files',\n",
       " 452: 'low',\n",
       " 135: 'close',\n",
       " 855: 'volume',\n",
       " 589: 'price',\n",
       " 30: 'activity',\n",
       " 794: 'timestamp',\n",
       " 230: 'did',\n",
       " 262: 'error',\n",
       " 661: 'reporting',\n",
       " 680: 'risk',\n",
       " 848: 'various',\n",
       " 459: 'making',\n",
       " 233: 'difficult',\n",
       " 694: 'scraping',\n",
       " 786: 'thank',\n",
       " 715: 'share',\n",
       " 858: 'way',\n",
       " 161: 'conducted',\n",
       " 766: 'survey',\n",
       " 158: 'comprehensive',\n",
       " 637: 'received',\n",
       " 420: 'learned',\n",
       " 874: 'working',\n",
       " 377: 'initial',\n",
       " 659: 'report',\n",
       " 716: 'shared',\n",
       " 494: 'multiple',\n",
       " 724: 'single',\n",
       " 681: 'row',\n",
       " 243: 'does',\n",
       " 629: 'rates',\n",
       " 536: 'package',\n",
       " 816: 'txt',\n",
       " 512: 'november',\n",
       " 491: 'month',\n",
       " 2: '1000',\n",
       " 864: 'week',\n",
       " 48: 'analyses',\n",
       " 633: 'read',\n",
       " 403: 'kind',\n",
       " 182: 'country',\n",
       " 495: 'named',\n",
       " 53: 'answer',\n",
       " 647: 'regarding',\n",
       " 669: 'required',\n",
       " 370: 'indicates',\n",
       " 438: 'list',\n",
       " 442: 'live',\n",
       " 76: 'august',\n",
       " 484: 'minutes',\n",
       " 802: 'total',\n",
       " 610: 'provided',\n",
       " 628: 'rate',\n",
       " 657: 'relevant',\n",
       " 310: 'generally',\n",
       " 759: 'students',\n",
       " 707: 'separated',\n",
       " 402: 'key',\n",
       " 300: 'free',\n",
       " 298: 'form',\n",
       " 145: 'come',\n",
       " 540: 'paper',\n",
       " 475: 'measurements',\n",
       " 594: 'problems',\n",
       " 821: 'uci',\n",
       " 663: 'repository',\n",
       " 686: 'samples',\n",
       " 607: 'properties',\n",
       " 228: 'development',\n",
       " 371: 'indicators',\n",
       " 83: 'bank',\n",
       " 632: 'raw',\n",
       " 32: 'actually',\n",
       " 211: 'deep',\n",
       " 507: 'nlp',\n",
       " 184: 'course',\n",
       " 561: 'play',\n",
       " 219: 'description',\n",
       " 163: 'considered',\n",
       " 203: 'date',\n",
       " 627: 'range',\n",
       " 752: 'stock',\n",
       " 81: 'average',\n",
       " 699: 'second',\n",
       " 435: 'lines',\n",
       " 248: 'downloaded',\n",
       " 235: 'directly',\n",
       " 375: 'info',\n",
       " 144: 'combined',\n",
       " 408: 'label',\n",
       " 523: 'ones',\n",
       " 774: 'task',\n",
       " 269: 'evaluation',\n",
       " 12: '2014',\n",
       " 783: 'test',\n",
       " 882: 'years',\n",
       " 740: 'split',\n",
       " 583: 'prediction',\n",
       " 789: 'think',\n",
       " 489: 'money',\n",
       " 285: 'feel',\n",
       " 749: 'stats',\n",
       " 735: 'special',\n",
       " 738: 'speed',\n",
       " 217: 'described',\n",
       " 566: 'points',\n",
       " 332: 'health',\n",
       " 25: 'acquired',\n",
       " 726: 'sites',\n",
       " 54: 'answered',\n",
       " 734: 'space',\n",
       " 470: 'mean',\n",
       " 884: 'zip',\n",
       " 137: 'codes',\n",
       " 516: 'observations',\n",
       " 229: 'dictionary',\n",
       " 187: 'create',\n",
       " 584: 'predictive',\n",
       " 486: 'model',\n",
       " 354: 'identify',\n",
       " 751: 'step',\n",
       " 805: 'train',\n",
       " 559: 'plan',\n",
       " 847: 'variety',\n",
       " 872: 'words',\n",
       " 677: 'review',\n",
       " 870: 'won',\n",
       " 576: 'post',\n",
       " 164: 'consists',\n",
       " 687: 'scale',\n",
       " 678: 'reviews',\n",
       " 691: 'score',\n",
       " 795: 'title',\n",
       " 218: 'describing',\n",
       " 648: 'region',\n",
       " 379: 'inside',\n",
       " 553: 'person',\n",
       " 815: 'twitter',\n",
       " 693: 'scraped',\n",
       " 829: 'update',\n",
       " 881: 'year',\n",
       " 705: 'sentiment',\n",
       " 785: 'text',\n",
       " 487: 'models',\n",
       " 533: 'overall',\n",
       " 117: 'change',\n",
       " 39: 'age',\n",
       " 713: 'sets',\n",
       " 320: 'goes',\n",
       " 447: 'long',\n",
       " 250: 'early',\n",
       " 853: 'visit',\n",
       " 860: 'weather',\n",
       " 780: 'temperature',\n",
       " 580: 'pre',\n",
       " 46: 'allows',\n",
       " 617: 'publish',\n",
       " 58: 'applied',\n",
       " 481: 'methods',\n",
       " 45: 'allow',\n",
       " 471: 'meaning',\n",
       " 469: 'maximum',\n",
       " 129: 'city',\n",
       " 297: 'food',\n",
       " 603: 'products',\n",
       " 34: 'added',\n",
       " 618: 'published',\n",
       " 758: 'structure',\n",
       " 833: 'url',\n",
       " 110: 'categories',\n",
       " 128: 'cities',\n",
       " 515: 'numeric',\n",
       " 366: 'increase',\n",
       " 688: 'school',\n",
       " 744: 'starting',\n",
       " 367: 'increased',\n",
       " 579: 'power',\n",
       " 20: 'according',\n",
       " 103: 'california',\n",
       " 756: 'street',\n",
       " 394: 'journal',\n",
       " 531: 'originally',\n",
       " 207: 'death',\n",
       " 121: 'characteristics',\n",
       " 220: 'descriptions',\n",
       " 178: 'corresponding',\n",
       " 280: 'family',\n",
       " 509: 'north',\n",
       " 47: 'american',\n",
       " 352: 'identified',\n",
       " 61: 'april',\n",
       " 549: 'perform',\n",
       " 356: 'image',\n",
       " 730: 'software',\n",
       " 62: 'archive',\n",
       " 349: 'ics',\n",
       " 72: 'attribute',\n",
       " 634: 'real',\n",
       " 741: 'standard',\n",
       " 443: 'local',\n",
       " 414: 'largest',\n",
       " 720: 'significant',\n",
       " 238: 'distribution',\n",
       " 16: '500',\n",
       " 520: 'october',\n",
       " 685: 'sample',\n",
       " 524: 'online',\n",
       " 426: 'levels',\n",
       " 465: 'march',\n",
       " 640: 'recognition',\n",
       " 568: 'policy',\n",
       " 796: 'today',\n",
       " 554: 'personal',\n",
       " 455: 'main',\n",
       " 866: 'weights',\n",
       " 266: 'estimates',\n",
       " 279: 'factors',\n",
       " 602: 'production',\n",
       " 765: 'support',\n",
       " 335: 'higher',\n",
       " 336: 'highest',\n",
       " 274: 'experience',\n",
       " 60: 'approximately',\n",
       " 702: 'seen',\n",
       " 413: 'large',\n",
       " 565: 'point',\n",
       " 102: 'calculated',\n",
       " 33: 'add',\n",
       " 66: 'article',\n",
       " 357: 'images',\n",
       " 44: 'algorithms',\n",
       " 727: 'size',\n",
       " 784: 'testing',\n",
       " 151: 'community',\n",
       " 514: 'numbers',\n",
       " 667: 'represents',\n",
       " 444: 'located',\n",
       " 422: 'left',\n",
       " 70: 'assigned',\n",
       " 174: 'converted',\n",
       " 428: 'license',\n",
       " 176: 'copyright',\n",
       " 762: 'subject',\n",
       " 433: 'limited',\n",
       " 78: 'authors',\n",
       " 27: 'action',\n",
       " 257: 'election',\n",
       " 591: 'primary',\n",
       " 183: 'county',\n",
       " 261: 'entire',\n",
       " 436: 'link',\n",
       " 425: 'level',\n",
       " 96: 'book',\n",
       " 122: 'characters',\n",
       " 120: 'character',\n",
       " 188: 'created',\n",
       " 345: 'html',\n",
       " 528: 'org',\n",
       " 156: 'compiled',\n",
       " 101: 'business',\n",
       " 308: 'gender',\n",
       " 307: 'gathered',\n",
       " 714: 'sex',\n",
       " 258: 'end',\n",
       " 704: 'self',\n",
       " 460: 'male',\n",
       " 286: 'female',\n",
       " 578: 'potential',\n",
       " 314: 'getting',\n",
       " 90: 'better',\n",
       " 616: 'publicly',\n",
       " 550: 'performance',\n",
       " 98: 'building',\n",
       " 773: 'target',\n",
       " 453: 'lower',\n",
       " 216: 'derived',\n",
       " 180: 'count',\n",
       " 343: 'hours',\n",
       " 389: 'involved',\n",
       " 837: 'useful',\n",
       " 9: '2011',\n",
       " 698: 'season',\n",
       " 562: 'played',\n",
       " 138: 'collect',\n",
       " 479: 'metadata',\n",
       " 480: 'method',\n",
       " 368: 'index',\n",
       " 473: 'measure',\n",
       " 265: 'estimated',\n",
       " 590: 'prices',\n",
       " 291: 'final',\n",
       " 588: 'previously',\n",
       " 185: 'cover',\n",
       " 827: 'units',\n",
       " 225: 'determine',\n",
       " 212: 'defined',\n",
       " 149: 'common',\n",
       " 112: 'census',\n",
       " 496: 'names',\n",
       " 500: 'negative',\n",
       " 877: 'worth',\n",
       " 365: 'income',\n",
       " 450: 'looking',\n",
       " 382: 'instead',\n",
       " 723: 'simply',\n",
       " 824: 'unique',\n",
       " 344: 'house',\n",
       " 650: 'regression',\n",
       " 396: 'july',\n",
       " 655: 'release',\n",
       " 204: 'dates',\n",
       " 412: 'languages',\n",
       " 437: 'links',\n",
       " 355: 'ids',\n",
       " 763: 'subset',\n",
       " 57: 'applications',\n",
       " 803: 'track',\n",
       " 857: 'wanted',\n",
       " 339: 'history',\n",
       " 755: 'story',\n",
       " 767: 'systems',\n",
       " 804: 'traffic',\n",
       " 197: 'daily',\n",
       " 153: 'company',\n",
       " 278: 'extracted',\n",
       " 570: 'popular',\n",
       " 200: 'databases',\n",
       " 504: 'neural',\n",
       " 503: 'networks',\n",
       " 743: 'started',\n",
       " 638: 'recent',\n",
       " 441: 'little',\n",
       " 787: 'thanks',\n",
       " 87: 'basis',\n",
       " 771: 'taken',\n",
       " 147: 'comments',\n",
       " 502: 'network',\n",
       " 654: 'relative',\n",
       " 548: 'percentage',\n",
       " 240: 'divided',\n",
       " 710: 'service',\n",
       " 555: 'photo',\n",
       " 809: 'trend',\n",
       " 832: 'uploaded',\n",
       " 797: 'took',\n",
       " 770: 'tags',\n",
       " 260: 'english',\n",
       " 190: 'creative',\n",
       " 150: 'commons',\n",
       " 493: 'months',\n",
       " 711: 'services',\n",
       " 597: 'processed',\n",
       " 769: 'tables',\n",
       " 718: 'short',\n",
       " 323: 'google',\n",
       " 859: 'ways',\n",
       " 690: 'scientific',\n",
       " 790: 'thought',\n",
       " 415: 'later',\n",
       " 639: 'recently',\n",
       " 636: 'reasons',\n",
       " 798: 'tool',\n",
       " 64: 'areas',\n",
       " 682: 'rows',\n",
       " 879: 'written',\n",
       " 268: 'evaluate',\n",
       " 411: 'language',\n",
       " 558: 'places',\n",
       " 793: 'times',\n",
       " 256: 'education',\n",
       " 476: 'measures',\n",
       " 383: 'institute',\n",
       " 301: 'frequency',\n",
       " 50: 'analyze',\n",
       " 107: 'care',\n",
       " 604: 'program',\n",
       " 673: 'resources',\n",
       " 253: 'easy',\n",
       " 478: 'medical',\n",
       " 556: 'physical',\n",
       " 511: 'notes',\n",
       " 313: 'geographic',\n",
       " 517: 'obtain',\n",
       " 717: 'sharing',\n",
       " 458: 'makes',\n",
       " 472: 'means',\n",
       " 868: 'wikipedia',\n",
       " 800: 'topic',\n",
       " 811: 'true',\n",
       " 105: 'came',\n",
       " 427: 'library',\n",
       " 133: 'classify',\n",
       " 814: 'tweets',\n",
       " 612: 'providing',\n",
       " 737: 'speech',\n",
       " 348: 'human',\n",
       " 474: 'measured',\n",
       " 259: 'energy',\n",
       " 231: 'difference',\n",
       " 651: 'regular',\n",
       " 646: 'references',\n",
       " 177: 'corpus',\n",
       " 134: 'cleaned',\n",
       " 77: 'author',\n",
       " 567: 'police',\n",
       " 342: 'hour',\n",
       " 29: 'activities',\n",
       " 387: 'internet',\n",
       " 653: 'relationship',\n",
       " 595: 'proceedings',\n",
       " 162: 'conference',\n",
       " 111: 'category',\n",
       " 643: 'records',\n",
       " 551: 'performed',\n",
       " 93: 'blog',\n",
       " 391: 'items',\n",
       " 778: 'techniques',\n",
       " 854: 'visualization',\n",
       " 747: 'statistical',\n",
       " 330: 'hard',\n",
       " 635: 'really',\n",
       " 417: 'law',\n",
       " 65: 'art',\n",
       " 775: 'tasks',\n",
       " 329: 'hand',\n",
       " 865: 'weight',\n",
       " 196: 'currently',\n",
       " 739: 'spent',\n",
       " 560: 'platform',\n",
       " 4: '200',\n",
       " 284: 'federal',\n",
       " 538: 'pages',\n",
       " 242: 'documents',\n",
       " 236: 'distance',\n",
       " 492: 'monthly',\n",
       " 630: 'rating',\n",
       " 852: 'vision',\n",
       " 883: 'york',\n",
       " 392: 'january',\n",
       " 546: 'pdf',\n",
       " 416: 'latitude',\n",
       " 448: 'longitude',\n",
       " 223: 'detect',\n",
       " 823: 'understanding',\n",
       " 593: 'problem',\n",
       " 51: 'analyzing',\n",
       " 409: 'labeled',\n",
       " 665: 'represented',\n",
       " 875: 'works',\n",
       " 213: 'demographic',\n",
       " 263: 'especially',\n",
       " 116: 'challenge',\n",
       " 341: 'hope',\n",
       " 186: 'covers',\n",
       " 592: 'prior',\n",
       " 541: 'papers',\n",
       " 42: 'air',\n",
       " 79: 'automatically',\n",
       " 446: 'locations',\n",
       " 245: 'domain',\n",
       " 95: 'body',\n",
       " 124: 'children',\n",
       " 806: 'trained',\n",
       " 125: 'citation',\n",
       " 293: 'focus',\n",
       " 317: 'gives',\n",
       " 501: 'net',\n",
       " 227: 'developed',\n",
       " 155: 'competition',\n",
       " 189: 'creating',\n",
       " 498: 'natural',\n",
       " 106: 'car',\n",
       " 331: 'having',\n",
       " 324: 'gov',\n",
       " 26: 'act',\n",
       " 625: 'race',\n",
       " 791: 'thousands',\n",
       " 672: 'resource',\n",
       " 753: 'stop',\n",
       " 543: 'parts',\n",
       " 725: 'site',\n",
       " 94: 'board',\n",
       " 59: 'approach',\n",
       " 173: 'control',\n",
       " 463: 'map',\n",
       " 215: 'depth',\n",
       " 571: 'population',\n",
       " 192: 'crime',\n",
       " 277: 'extract',\n",
       " 813: 'trying',\n",
       " 645: 'reference',\n",
       " 5: '2000',\n",
       " 209: 'decided',\n",
       " 100: 'bureau',\n",
       " 754: 'stored',\n",
       " 539: 'pairs',\n",
       " 464: 'maps',\n",
       " 126: 'citations',\n",
       " 462: 'manually',\n",
       " 159: 'computer',\n",
       " 68: 'arxiv',\n",
       " 429: 'licensed',\n",
       " 819: 'typical',\n",
       " 703: 'selected',\n",
       " 572: 'portal',\n",
       " 620: 'purposes',\n",
       " 760: 'studies',\n",
       " 226: 'develop',\n",
       " 404: 'kindly',\n",
       " 28: 'active',\n",
       " 781: 'term',\n",
       " 353: 'identifier',\n",
       " 208: 'december',\n",
       " 820: 'typically',\n",
       " 733: 'south',\n",
       " 871: 'word',\n",
       " 239: 'district',\n",
       " 244: 'doi',\n",
       " 40: 'agencies',\n",
       " 38: 'administration',\n",
       " 423: 'length',\n",
       " 757: 'string',\n",
       " 369: 'india',\n",
       " 210: 'decision',\n",
       " 808: 'tree',\n",
       " 92: 'bigger',\n",
       " 74: 'attribution',\n",
       " 3: '1st',\n",
       " 294: 'follow',\n",
       " 526: 'opportunity',\n",
       " 490: 'monitoring',\n",
       " 303: 'function',\n",
       " 878: 'wouldn',\n",
       " 88: 'benefit',\n",
       " 535: 'owe',\n",
       " 75: 'attributions'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map = dict((v, k) for k, v in vectorizer.vocabulary_.items())\n",
    "id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=6, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.066*\"data\" + 0.020*\"context\" + 0.019*\"content\" + 0.017*\"inspiration\" + 0.017*\"acknowledgements\" + 0.014*\"world\" + 0.013*\"dataset\" + 0.013*\"research\" + 0.012*\"day\" + 0.011*\"weather\"'),\n",
       " (1,\n",
       "  '0.080*\"dataset\" + 0.033*\"description\" + 0.033*\"does\" + 0.016*\"data\" + 0.015*\"images\" + 0.013*\"class\" + 0.013*\"contains\" + 0.012*\"different\" + 0.012*\"column\" + 0.011*\"image\"'),\n",
       " (2,\n",
       "  '0.041*\"data\" + 0.019*\"csv\" + 0.013*\"dataset\" + 0.012*\"information\" + 0.010*\"year\" + 0.008*\"health\" + 0.008*\"number\" + 0.008*\"states\" + 0.008*\"state\" + 0.007*\"code\"'),\n",
       " (3,\n",
       "  '0.359*\"university\" + 0.069*\"state\" + 0.033*\"trained\" + 0.027*\"model\" + 0.021*\"pre\" + 0.019*\"california\" + 0.015*\"features\" + 0.015*\"dataset\" + 0.014*\"institute\" + 0.011*\"data\"'),\n",
       " (4,\n",
       "  '0.047*\"data\" + 0.022*\"time\" + 0.019*\"2017\" + 0.018*\"dataset\" + 0.018*\"date\" + 0.016*\"content\" + 0.014*\"number\" + 0.013*\"context\" + 0.013*\"times\" + 0.012*\"acknowledgements\"'),\n",
       " (5,\n",
       "  '0.043*\"data\" + 0.034*\"dataset\" + 0.015*\"com\" + 0.014*\"content\" + 0.013*\"contains\" + 0.012*\"context\" + 0.011*\"acknowledgements\" + 0.010*\"csv\" + 0.010*\"using\" + 0.010*\"file\"')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=6, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the new text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_names = ['Education', 'Computers & IT', 'Religion', 'Sports', 'Science', 'Society & Lifestyle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "It's my understanding that the freezing will start to occur because of the\n",
      "growing distance of Pluto and Charon from the Sun, due to it's\n",
      "elliptical orbit. It is not due to shadowing effects. \n",
      "\n",
      "\n",
      "Pluto can shadow Charon, and vice-versa.\n",
      "\n",
      "George Krumins\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]\n",
    "print(new_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.04166859),\n",
       "  (1, 0.041689582),\n",
       "  (2, 0.041702054),\n",
       "  (3, 0.0416686),\n",
       "  (4, 0.79156804),\n",
       "  (5, 0.041703176)]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vectorized = vectorizer.transform(new_doc)  \n",
    "\n",
    "new_doc_corpus = gensim.matutils.Sparse2Corpus(doc_vectorized, documents_columns=False)\n",
    "doc_topics = ldamodel.get_document_topics(new_doc_corpus)\n",
    "list(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Science'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elicit_topic_name(doc_topics):\n",
    "    return topics_names[np.squeeze(np.array(doc_topics))[:, 1].argmax()]\n",
    "\n",
    "elicit_topic_name(doc_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
